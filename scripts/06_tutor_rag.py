#ë””ë²„ê¹…ìš©
DEBUG_MODE = True # Trueë¡œ ì„¤ì •í•˜ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥

def log_debug(message: str):
    """ë””ë²„ê·¸ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    if DEBUG_MODE:
        print(f"ğŸ› DEBUG: {message}")

import os
import json
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_neo4j import Neo4jGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from utils.student_profile import load_profile, save_profile

load_dotenv()

NEO4J_URI = os.getenv('NEO4J_URI')
NEO4J_USER = os.getenv('NEO4J_USER')
NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')

#LLM, graphDB ì´ˆê¸°í™”
llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.3)
graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASSWORD)


#1. ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê°œë… ì¶”ì¶œ
def extract_concept(user_question: str) -> str:
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì¤‘í•™êµ ìˆ˜í•™ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì—ì„œ í•™ìƒì´ ê¶ê¸ˆí•´í•˜ëŠ” 'í•µì‹¬ ìˆ˜í•™ ê°œë…'ì„ ì¶”ì¶œí•˜ì„¸ìš”.
ë°˜ë“œì‹œ ê°œë… ì´ë¦„ë§Œ ë°˜í™˜í•˜ê³ , ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.

ê·œì¹™:
1. ê°œë…ì„ *ì •í™•íˆ* ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 'ê°ë¿”ëŒ€'ë¥¼ 'ê°ë¿”'ë¡œ ì¶”ì¶œí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.)
2. ì§ˆë¬¸ì´ ê°œë… ê·¸ ìì²´ì¸ ê²½ìš°, í•´ë‹¹ ê°œë…ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
3. ë‘ ê°œë…ì˜ 'ì°¨ì´'ë‚˜ 'ë¹„êµ'ë¥¼ ë¬»ëŠ” ê²½ìš°, 'Aì™€ B' í˜•ì‹ìœ¼ë¡œ ë‘ ê°œë…ì„ ëª¨ë‘ ë°˜í™˜í•˜ì„¸ìš”.
4. **(ì‹ ê·œ) 'ë„“ì´', 'ë¶€í”¼', 'êµ¬í•˜ëŠ” ë²•' ë“± ì†ì„±ì´ë‚˜ ë°©ë²•ì„ ë¬»ëŠ” ê²½ìš°, ì´ë¥¼ í¬í•¨í•˜ì—¬ ì¶”ì¶œí•˜ì„¸ìš”.**

ì˜ˆì‹œ:
ì§ˆë¬¸: "ì¼ì°¨ë°©ì •ì‹ì´ ë­ì•¼?" â†’ ì¼ì°¨ë°©ì •ì‹
ì§ˆë¬¸: "ê³„ìˆ˜ë¥¼ ì–´ë–»ê²Œ êµ¬í•´?" â†’ ê³„ìˆ˜ êµ¬í•˜ëŠ” ë²•
ì§ˆë¬¸: "ì •ë¹„ë¡€ì™€ ë°˜ë¹„ë¡€ ì°¨ì´ê°€ ë­ì•¼?" â†’ ì •ë¹„ë¡€ì™€ ë°˜ë¹„ë¡€
ì§ˆë¬¸: "í•¨ìˆ˜ë‘ ë°©ì •ì‹ì´ë‘ ë­ê°€ ë‹¬ë¼?" â†’ í•¨ìˆ˜ì™€ ë°©ì •ì‹
ì§ˆë¬¸: "ê°ë¿”ëŒ€ê°€ ë­ì•¼?" â†’ ê°ë¿”ëŒ€
ì§ˆë¬¸: "ë¯¸ì ë¶„ì´ ë­ì•¼?" â†’ ë¯¸ì ë¶„
ì§ˆë¬¸: **"ê°ë¿”ì˜ ë¶€í”¼ëŠ” ë­ì•¼?" â†’ ê°ë¿”ì˜ ë¶€í”¼**
ì§ˆë¬¸: **"ì›ê¸°ë‘¥ ë„“ì´ ì–´ë–»ê²Œ êµ¬í•´?" â†’ ì›ê¸°ë‘¥ ë„“ì´ êµ¬í•˜ëŠ” ë²•**

ê°œë…ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ê°œë…ì—†ìŒ"ì´ë¼ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""),
        ("user", "{question}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    concept = chain.invoke({"question": user_question}).strip()
    return concept


# 2. ì„ ìˆ˜ ê°œë… ì°¾ê¸° (ê·¸ë˜í”„ íƒìƒ‰)
def get_prerequisites(concept_name: str, depth: int = 2) -> list:
    """ê°œë…ì˜ ì„ ìˆ˜ ì§€ì‹ì„ depth ë‹¨ê³„ë§Œí¼ ì°¾ê¸°"""
    query = f"""
    MATCH path = (prereq:CoreConcept)-[:IS_PREREQUISITE_OF*1..{depth}]->(target:CoreConcept {{name: $concept}})
    WITH prereq, length(path) AS dist
    RETURN DISTINCT prereq.name AS name, prereq.definition AS definition, dist
    ORDER BY dist
    """
    
    try:
        results = graph.query(query, params={"concept": concept_name})
        return [{"name": r["name"], "definition": r["definition"], "depth": r["dist"]} 
                for r in results]
    except Exception as e:
        print(f"âš ï¸ ê·¸ë˜í”„ íƒìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ì‹œê°í™”ë¥¼ ìœ„í•œ ê²½ë¡œ íƒìƒ‰ (ì‹ ê·œ)
def get_path_for_visualization(concept_name: str) -> dict:
    """
    ì‹œê°í™”ë¥¼ ìœ„í•´ íŠ¹ì • ê°œë…ì˜ ë¡œì»¬ í•™ìŠµ ê²½ë¡œ(ì„ ìˆ˜/í›„ì†)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    (streamlit-agraph í˜•ì‹ì— ë§ëŠ” ë…¸ë“œì™€ ì—£ì§€ ë°˜í™˜)
    """
    query = """
    MATCH (target:CoreConcept {name: $concept})
    // 1. ì„ ìˆ˜ ê°œë… (2ë‹¨ê³„ ë’¤ê¹Œì§€)
    OPTIONAL MATCH path_prereq = (prereq:CoreConcept)-[:IS_PREREQUISITE_OF*1..2]->(target)
    // 2. í›„ì† ê°œë… (1ë‹¨ê³„ ì•ê¹Œì§€)
    OPTIONAL MATCH path_dep = (target)-[:IS_PREREQUISITE_OF*1..1]->(dependent:CoreConcept)
    
    // ëª¨ë“  ë…¸ë“œì™€ ê´€ê³„ ìˆ˜ì§‘
    WITH target, 
         collect(nodes(path_prereq)) + collect(nodes(path_dep)) AS node_lists,
         collect(relationships(path_prereq)) + collect(relationships(path_dep)) AS rel_lists
    
    // ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ í’€ì–´ì„œ ìœ ë‹ˆí¬í•˜ê²Œ ë§Œë“¤ê¸°
    UNWIND node_lists AS node_list
    UNWIND node_list AS n
    WITH target, collect(DISTINCT n) AS all_nodes, rel_lists
    
    // ê´€ê³„ ë¦¬ìŠ¤íŠ¸ë¥¼ í’€ì–´ì„œ ìœ ë‹ˆí¬í•˜ê²Œ ë§Œë“¤ê¸°
    UNWIND rel_lists AS rel_list
    UNWIND rel_list AS r
    WITH all_nodes + target AS final_nodes_list, collect(DISTINCT r) AS final_rels
    
    // ìµœì¢… ë…¸ë“œ/ì—£ì§€ í¬ë§·íŒ…
    WITH [n IN final_nodes_list | {id: n.name, label: n.name}] AS nodes,
         [r IN final_rels | {source: startNode(r).name, target: endNode(r).name, label: 'ì„ ìˆ˜ê°œë…'}] AS edges
         
    RETURN nodes, edges
    """
    try:
        results = graph.query(query, params={"concept": concept_name})
        if results and results[0]["nodes"]:
            log_debug(f"'{concept_name}'ì˜ ì‹œê°í™” ê²½ë¡œ ì¡°íšŒ ì„±ê³µ")
            return {
                "nodes": results[0]["nodes"],
                "edges": results[0]["edges"]
            }
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ê²½ë¡œ íƒìƒ‰ ì˜¤ë¥˜: {e}")
    
    return {"nodes": [], "edges": []}

# ì§„ë‹¨ ì§ˆë¬¸ ìƒì„±
def generate_diagnostic_question(target_concept: str, prerequisites: list):
    """ì„ ìˆ˜ ê°œë…ì„ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¸í•˜ëŠ” ì§ˆë¬¸ ìƒì„± (ìŠ¤íŠ¸ë¦¼ ë°˜í™˜)"""
    if not prerequisites:
        return None
    
    # ê°€ì¥ ê°€ê¹Œìš´ ì„ ìˆ˜ ê°œë… (depth=1)ë§Œ ì‚¬ìš©
    immediate_prereqs = [p for p in prerequisites if p["depth"] == 1]
    if not immediate_prereqs:
        return None
    
    prereq_info = "\n".join([f"- {p['name']}: {p['definition']}" for p in immediate_prereqs])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì´ '{target_concept}'ì„ ë¬¼ì–´ë´¤ì„ ë•Œ, ì´ ê°œë…ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë¨¼ì € ì•Œì•„ì•¼ í•  ì„ ìˆ˜ ì§€ì‹ì„ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

ê·œì¹™:
1. í•™ìƒì˜ ê¸°ë¶„ì„ ìƒí•˜ê²Œ í•˜ì§€ ë§ê³ , ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”
2. "í˜¹ì‹œ ê¸°ì–µë‚˜ì‹œë‚˜ìš”?", "ë¨¼ì € í™•ì¸í•´ë³¼ê¹Œìš”?" ê°™ì€ ë¶€ë“œëŸ¬ìš´ í‘œí˜„ ì‚¬ìš©
3. ì„ ìˆ˜ ê°œë… 1~2ê°œë§Œ ì–¸ê¸‰ (ë„ˆë¬´ ë§ìœ¼ë©´ ë¶€ë‹´)
4. ì§ˆë¬¸ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ

ì˜ˆì‹œ:
"ì¢‹ì€ ì§ˆë¬¸ì´ì—ìš”! ì¼ì°¨ë°©ì •ì‹ì„ ì œëŒ€ë¡œ ì´í•´í•˜ë ¤ë©´ 'ë°©ì •ì‹'ê³¼ 'ì¼ì°¨ì‹' ê°œë…ë¶€í„° í™•ì¸í•´ë³´ë©´ ì¢‹ì€ë°, í˜¹ì‹œ ì´ ê°œë…ë“¤ì€ ê¸°ì–µë‚˜ì‹œë‚˜ìš”?"
"""),
        ("user", """ëª©í‘œ ê°œë…: {target_concept}
ì„ ìˆ˜ ê°œë…ë“¤:
{prereq_info}

ìœ„ ì„ ìˆ˜ ê°œë…ì„ í™•ì¸í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.""")
    ])
    
    chain = prompt | llm | StrOutputParser()
    return chain.stream({
        "target_concept": target_concept,
        "prereq_info": prereq_info
    })

# 4. ì´í•´ë„ íŒë‹¨
def assess_understanding(user_response: str, prereq_names: list) -> dict:
    """í•™ìƒ ë‹µë³€ì„ ë³´ê³  ê° ì„ ìˆ˜ ê°œë…ë³„ ì´í•´ ì—¬ë¶€ íŒë‹¨"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ í•™ìƒì˜ ì´í•´ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•™ìƒì´ ì—¬ëŸ¬ ê°œë…ì— ëŒ€í•´ ë‹µë³€í–ˆì„ ë•Œ, **ê° ê°œë…ë³„ë¡œ** ì´í•´ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

íŒë‹¨ ê¸°ì¤€:
- ëª…í™•í•œ ê¸ì • (ì•Œì•„ìš”, ì´í•´í•´ìš”, ì‘, ë„¤, ê·¸ë˜, ë§ì•„ ë“±) â†’ true
- ëª…í™•í•œ ë¶€ì • (ëª°ë¼ìš”, ëª¨ë¥´ê² ì–´ìš”, ì•„ë‹ˆìš”, ì•„ë‹ˆ, ê¸°ì–µ ì•ˆë‚˜ ë“±) â†’ false
- ì• ë§¤í•œ í‘œí˜„ / ì–¸ê¸‰ ì—†ìŒ / ìœ„ ê¸ì •/ë¶€ì •ì— í•´ë‹¹ ì•ˆ ë¨ â†’ null

**ì¤‘ìš”**: í•™ìƒì´ "ì•„ë‹ˆ", "ì‘" ì´ë¼ê³ ë§Œ ë‹µí•´ë„ ê°ê° false/trueë¡œ ëª…í™•íˆ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤!

**ì˜ˆì‹œ**:
"AëŠ” ì•Œì•„ìš”, BëŠ” ëª¨ë¥´ê² ì–´ìš”" â†’ {{"A": true, "B": false}}
"Aê°€ ë­ì˜€ë”ë¼" â†’ {{"A": false, (ë‹¤ë¥¸ ê°œë…): null}}
"ì‘" â†’ (ëª¨ë“  ì–¸ê¸‰ëœ ê°œë…): true
"ì•„ë‹ˆ" â†’ (ëª¨ë“  ì–¸ê¸‰ëœ ê°œë…): false

ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
{{"ê°œë…1": false, "ê°œë…2": true, "ê°œë…3": null}}
"""),
        ("user", """ì„ ìˆ˜ ê°œë…ë“¤: {prereq_names}
í•™ìƒ ë‹µë³€: {response}

ê° ê°œë…ë³„ ì´í•´ ì—¬ë¶€ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.""")
    ])

    chain = prompt | llm | StrOutputParser()
    result_str = chain.invoke({
        "prereq_names": prereq_names,
        "response": user_response
    }).strip()

    try:
        understanding_map = json.loads(result_str)
        for name in prereq_names:
            if name not in understanding_map:
                understanding_map[name] = None
        return understanding_map
    except Exception as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"   LLM ì‘ë‹µ: {result_str}")
        return {name: None for name in prereq_names}

# 5. ê·¸ë˜í”„ì—ì„œ ê°œë… ì •ë³´ (ì •ì˜, ê´€ë ¨ ì˜ˆì‹œ) ê°€ì ¸ì˜¤ê¸°
def retrieve_concept_from_graph(concept_name: str) -> dict:
    core_query = """
    MATCH (c:CoreConcept {name: $name}) 
    RETURN c.name AS name, c.definition AS definition
    """
    core_result = graph.query(core_query, params={"name": concept_name})
    
    if not core_result:
        return None
    
    example_query = """
    MATCH (concept:Concept)-[:IS_EXAMPLE_OF]->(core:CoreConcept {name: $name})
    RETURN concept.definition AS example
    LIMIT 3
    """
    examples = graph.query(example_query, params={"name": concept_name})
    
    return {
        "name": core_result[0]["name"],
        "definition": core_result[0]["definition"],
        "examples": [ex["example"] for ex in examples] if examples else []
    }
    

# 6. ë§ì¶¤ ì„¤ëª… ìƒì„± (streamëª¨ë“œ)
def generate_explanation(concept_info: dict, count: int = 0):
    """ê·¸ë˜í”„ ë°ì´í„° ê¸°ë°˜ ì‰¬ìš´ ì„¤ëª… ìƒì„± (ìŠ¤íŠ¸ë¦¼ ë°˜í™˜)"""
    concept_name = concept_info["name"]
    definition = concept_info["definition"]
    examples = concept_info.get("examples", [])
    
    examples_text = "\n".join([f"- {ex}" for ex in examples]) if examples else "ì˜ˆì‹œ ì—†ìŒ"
    
    system_message = """ë‹¹ì‹ ì€ ì¤‘í•™ìƒ ëˆˆë†’ì´ì— ë§ì¶° ì„¤ëª…í•˜ëŠ” ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì •ì˜ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
2. êµ¬ì²´ì ì¸ ì˜ˆì‹œ í¬í•¨ (ìˆ«ì ì˜ˆì‹œ)
3. 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
4. ê²©ë ¤í•˜ëŠ” ë§ë¡œ ë§ˆë¬´ë¦¬

ì˜ˆì‹œ:
"ê³„ìˆ˜ëŠ” ë¬¸ì ì•ì— ë¶™ëŠ” ìˆ«ìë¥¼ ë§í•´ìš”. ì˜ˆë¥¼ ë“¤ì–´ 3xì—ì„œ 3ì´ ê³„ìˆ˜ì˜ˆìš”. 
ì‚¬ê³¼ 3ê°œì²˜ëŸ¼ 'ëª‡ ê°œ'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìë¼ê³  ìƒê°í•˜ë©´ ì‰¬ì›Œìš”. 
ì´ì œ ì´í•´ë˜ì…¨ë‚˜ìš”?"
"""
    user_message_template = """ê°œë…: {concept_name}
ì •ì˜: {definition}

ê´€ë ¨ ì˜ˆì‹œ:
{examples}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‰¬ìš´ ì„¤ëª…ì„ ìƒì„±í•˜ì„¸ìš”."""

    if count > 0:
        system_message = f"""ë‹¹ì‹ ì€ ë§¤ìš° ì¸ë‚´ì‹¬ì´ ë§ì€ ì¤‘í•™êµ ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì´ ì´ì „ì— '{concept_name}' ê°œë…ì— ëŒ€í•œ ì„¤ëª…ì„ ë“¤ì—ˆì§€ë§Œ, ì—¬ì „íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

**ë°˜ë“œì‹œ ì´ì „ê³¼ ë‹¤ë¥¸ ë°©ì‹**ìœ¼ë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
- **ìƒˆë¡­ê³  ë” ì‰¬ìš´ ì˜ˆì‹œ**ë‚˜ **ë‹¤ë¥¸ ë¹„ìœ **ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ì ˆëŒ€ ì´ì „ì— í–ˆë˜ ë§(ì˜ˆ: "{definition}")ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
- 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ì§€ë§Œ, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”."""
        
        user_message_template = """ê°œë…: {concept_name}
ì •ì˜: {definition}
ê´€ë ¨ ì˜ˆì‹œ: {examples}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ìƒˆë¡­ê³  ì™„ì „íˆ ë‹¤ë¥¸ ë°©ì‹ì˜ ì„¤ëª…**ì„ ìƒì„±í•˜ì„¸ìš”."""

    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("user", user_message_template)
    ])
    
    chain = prompt | llm | StrOutputParser()
    
    return chain.stream({
        "concept_name": concept_info["name"],
        "definition": concept_info["definition"],
        "examples": concept_info.get("examples", [])
    })

# 6-1. ì¼ë°˜ ì„¤ëª… ìƒì„± í•¨ìˆ˜ (Fallbackìš©, ìŠ¤íŠ¸ë¦¬ë°)
def generate_general_explanation(concept_name: str):
    """LLMì˜ ì¼ë°˜ ì§€ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°œë…ì„ ì„¤ëª…í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¼ ë°˜í™˜)"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ ì¤‘í•™ìƒ ëˆˆë†’ì´ì— ë§ì¶° ìˆ˜í•™ ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ì¹œì ˆí•œ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì´ '{concept_name}'ì— ëŒ€í•´ ì§ˆë¬¸í–ˆì§€ë§Œ, ì´ ê°œë…ì€ ë‹¹ì‹ ì˜ ì „ë¬¸ ì§€ì‹ ê·¸ë˜í”„ì— ì•„ì§ ì—†ìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ '{concept_name}' ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì¤‘í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
2. ì˜ˆì‹œë¥¼ í¬í•¨í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
3. 3-5 ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
4. **ë§¤ìš° ì¤‘ìš”:** ì„¤ëª… ì‹œì‘ ë¶€ë¶„ì— **"(ì´ ì„¤ëª…ì€ ì œ ì§€ì‹ ê·¸ë˜í”„ì— ê¸°ë°˜í•œ ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë°˜ì ì¸ ë‚´ìš©ì´ì—ìš”.)"** ë¼ëŠ” ë©´ì±… ì¡°í•­(disclaimer)ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
"""),
        ("user", f"'{concept_name}' ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
    ])
    
    chain = prompt | llm | StrOutputParser()
    explanation = chain.stream({}) 
    log_debug(f"'{concept_name}'ì— ëŒ€í•œ ì¼ë°˜ ì„¤ëª… ìƒì„± ì™„ë£Œ.")
    return explanation

# 6-2. ë¬¸ì œ ìƒì„± í•¨ìˆ˜ (ìˆ˜ì •)
def generate_problem(concept_name: str, explanation_count: int) -> dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê°œë…ì— ëŒ€í•œ ë¬¸ì œ, ì •ë‹µ, í•µì‹¬ ì„ ìˆ˜ ê°œë…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    (ìˆ˜ì •) í•™ìƒì—ê²Œ ë³´ë‚¼ ìŠ¤íŠ¸ë¦¼ê³¼, ì •ë‹µ/í•µì‹¬ê°œë… ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # (ì‹ ê·œ) ë¬¸ì œ/ì •ë‹µ/í•µì‹¬ê°œë…ì„ JSONìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì²´ì¸
    # (ì‹ ê·œ) ì„¤ëª… íšŸìˆ˜ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•  ë¬¸ë§¥ ìƒì„±
    if explanation_count > 0:
        history_context = f"í•™ìƒì´ ì´ ê°œë…({concept_name})ì— ëŒ€í•œ ë¬¸ì œë¥¼ ì´ë¯¸ í’€ì–´ë³¸ ì ì´ ìˆìŠµë‹ˆë‹¤. **ë°˜ë“œì‹œ ì´ì „ê³¼ ë‹¤ë¥¸ ìƒˆë¡œìš´ ë¬¸ì œ**ë¥¼ ì¶œì œí•˜ì„¸ìš”."
    else:
        history_context = f"í•™ìƒì´ ì´ ê°œë…({concept_name})ì„ ë°©ê¸ˆ í•™ìŠµí–ˆìŠµë‹ˆë‹¤."
        
    problem_gen_prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ JSON ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
{history_context}
'{concept_name}' ê°œë…ì„ í™œìš©í•˜ëŠ” ê°„ë‹¨í•œ ë‹¨ë‹µí˜• ë¬¸ì œ 1ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

[ê·œì¹™]
1. ë°˜ë“œì‹œ 'problem', 'answer', 'key_concept'ë¼ëŠ” ì˜ì–´ í‚¤(key) 3ê°œë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
2. ì ˆëŒ€ ì‘ë‹µì„ ` ```json ... ``` ` (ë§ˆí¬ë‹¤ìš´)ìœ¼ë¡œ ê°ì‹¸ì§€ ë§ˆì„¸ìš”.
3. "problem" ê°’ì—ëŠ” ì¤„ë°”ê¿ˆì´ í•„ìš”í•˜ë©´ ë°˜ë“œì‹œ \\n ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
4. **(ìˆ˜ì •) "key_concept"ì—ëŠ” ì´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë° í•„ìš”í•œ '{concept_name}'ì˜ *ê°€ì¥ ì¤‘ìš”í•œ ì„ ìˆ˜ ê°œë…* 1ê°€ì§€ë¥¼ ì ìœ¼ì„¸ìš”.** (ì˜ˆ: 'ì´í•­', 'ë°‘ë©´ì˜ ë„“ì´', 'í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬'). ë§Œì•½ ë§ˆë•…í•œ ì„ ìˆ˜ ê°œë…ì´ ì—†ìœ¼ë©´ "none"ì´ë¼ê³  ì ìœ¼ì„¸ìš”.

[JSON í˜•ì‹]
{{{{
  "problem": "...",
  "answer": "...",
  "key_concept": "..."
}}}}

[JSON ì˜ˆì‹œ 1: ì¼ì°¨ë°©ì •ì‹ ë¬¸ì œ]
{{{{
  "problem": "... 2x + 3 = 11 ...",
  "answer": "4",
  "key_concept": "ì´í•­"
}}}}

[JSON ì˜ˆì‹œ 2: ê°ë¿” ë¶€í”¼ ë¬¸ì œ]
{{{{
  "problem": "... ê°ë¿”ì˜ ë¶€í”¼ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
  "answer": "120cmÂ³",
  "key_concept": "ë°‘ë©´ì˜ ë„“ì´"
}}}}
"""),
        # (ìˆ˜ì •) user ë©”ì‹œì§€ëŠ” ê°„ë‹¨í•˜ê²Œ
        ("user", f"'{concept_name}'ì— ëŒ€í•œ ë¬¸ì œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ 1ê°œ ì¶œì œí•´ì£¼ì„¸ìš”.")
    ])
    chain = problem_gen_prompt | llm
    
    try:
        # (ìˆ˜ì •) invoke ì‹œ ë³€ìˆ˜ë¥¼ ì „ë‹¬
        response_content = chain.invoke({
            "concept_name": concept_name,
            "history_context": history_context
        }).content
        log_debug(f"ë¬¸ì œ ìƒì„± JSON ì‘ë‹µ: {response_content}")
        
        # (ì‹ ê·œ) LLM ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒŒì‹± ì‹œë„
        data = json.loads(response_content)
        
        problem_text = data.get("problem", "ì˜¤ë¥˜: ë¬¸ì œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        problem_answer = data.get("answer", "none")
        problem_key_concept = data.get("key_concept", "none")
        
        # (ì‹ ê·œ) í•™ìƒì—ê²Œ ë³´ë‚¼ problem_textë§Œ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜
        problem_stream = iter([problem_text])
        
        return {
            "problem_stream": problem_stream,
            "problem_data": {
                "answer": problem_answer,
                "key_concept": problem_key_concept
            }
        }
        
    except Exception as e:
        print(f"âš ï¸ ë¬¸ì œ ìƒì„± JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        # (ì‹ ê·œ) ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•œ ë°˜í™˜
        return {
            "problem_stream": iter(["ì£„ì†¡í•©ë‹ˆë‹¤, ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”."]),
            "problem_data": None
        }

# 6-3. ì¡ë‹´ ì²˜ë¦¬
def handle_chitchat(user_input: str):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì¡ë‹´ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¼ ë°˜í™˜)"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ 'ìˆ˜í•™ íŠœí„°' ì±—ë´‡ì…ë‹ˆë‹¤. í•™ìƒì´ ìˆ˜í•™ê³¼ ê´€ë ¨ ì—†ëŠ” ê°„ë‹¨í•œ ëŒ€í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
ì§§ê³  ê°„ê²°í•˜ê²Œ 'íŠœí„°'ë¡œì„œ ì‘ë‹µí•˜ê³ , ë‹¤ì‹œ ìˆ˜í•™ ì§ˆë¬¸ì„ í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
        
ì˜ˆì‹œ:
- í•™ìƒ: ë„ˆëŠ” ëˆ„êµ¬ì•¼? / íŠœí„°: ì €ëŠ” AI ìˆ˜í•™ íŠœí„°ì…ë‹ˆë‹¤. ğŸ¤– ê¶ê¸ˆí•œ ìˆ˜í•™ ê°œë…ì„ ë¬¼ì–´ë³´ì„¸ìš”!
- í•™ìƒ: ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ? / íŠœí„°: ë‚ ì”¨ëŠ” ì˜ ëª¨ë¥´ì§€ë§Œ, ìˆ˜í•™ ê°œë…ì€ ë­ë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š
- í•™ìƒ: ê³ ë§ˆì›Œ / íŠœí„°: ì²œë§Œì—ìš”! ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‚˜ìš”?
"""),
        ("user", user_input)
    ])
    
    chain = prompt | llm | StrOutputParser()
    response = chain.stream({}) 
    log_debug("ì¡ë‹´ ì²˜ë¦¬ ì™„ë£Œ.")
    return response

# (handle_chitchat í•¨ìˆ˜ ì •ì˜ ë‹¤ìŒ)

# === 6-3b. ë¬¸ì œ í’€ì´ í”¼ë“œë°± ìƒì„± (ì‹ ê·œ) ===
def handle_solve_problem(user_answer: str, problem_data: dict):
    """
    í•™ìƒì˜ ë‹µì„ ì±„ì í•˜ê³  'ì§„ë‹¨í˜• í”¼ë“œë°±'ì„ ìƒì„±í•©ë‹ˆë‹¤. (ìŠ¤íŠ¸ë¦¼ ë°˜í™˜)
    """
    answer = problem_data.get("answer", "none")
    key_concept = problem_data.get("key_concept", "none")
    
    log_debug(f"ì±„ì  ì‹œì‘: í•™ìƒ ë‹µ={user_answer}, ì •ë‹µ={answer}, í•µì‹¬ê°œë…={key_concept}")

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ í•™ìƒì˜ ë‹µì„ ì±„ì í•˜ëŠ” ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì´ ë°©ê¸ˆ ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ì—ˆìŠµë‹ˆë‹¤. í•™ìƒì˜ ë‹µì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ , 'ì§„ë‹¨í˜• í”¼ë“œë°±'ì„ ì œê³µí•˜ì„¸ìš”.

[ë¬¸ì œ ì •ë³´]
- ì •ë‹µ: "{answer}"
- í•µì‹¬ ê°œë…: "{key_concept}" (ì´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë° í•„ìš”í–ˆë˜ ì„ ìˆ˜ ê°œë…)

[í”¼ë“œë°± ê·œì¹™]
1.  **ì •ë‹µì¼ ê²½ìš° (í•™ìƒì˜ ë‹µì´ "{answer}"ì™€ ì¼ì¹˜í•˜ê±°ë‚˜, "x= {answer}" ë“± ì˜ë¯¸ìƒ ê°™ì„ ê²½ìš°):**
    - "ì •ë‹µì…ë‹ˆë‹¤! ğŸ¥³"ë¼ê³  ì¹­ì°¬í•´ì£¼ì„¸ìš”.
    - ì´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë° ì‚¬ìš©ëœ **"{key_concept}"** ê°œë…ì„ ì˜ í™œìš©í–ˆë‹¤ê³  1~2ë¬¸ì¥ìœ¼ë¡œ ê²©ë ¤í•´ì£¼ì„¸ìš”.
    - (ì˜ˆ: "ì •ë‹µì…ë‹ˆë‹¤! ğŸ¥³ '+3'ì„ ë„˜ê¸°ëŠ” '{key_concept}' ê°œë…ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì…¨ë„¤ìš”. ì—­ì‹œ ê°œë…ì„ ì•„ë‹ˆê¹Œ ë¬¸ì œê°€ í’€ë¦¬ì£ ?")

2.  **ì˜¤ë‹µì¼ ê²½ìš° (...):**
    - **"ì•„ì‰½ë„¤ìš”, ì •ë‹µì€ '{answer}'ì˜€ì–´ìš”. ğŸ˜…"**ë¼ê³  **ì •ë‹µì„ ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”.**
    - ì´ ë¬¸ì œë¥¼ í’€ë ¤ë©´ **"{key_concept}"** ê°œë…ì´ í•„ìš”í–ˆë‹¤ê³  1~2ë¬¸ì¥ìœ¼ë¡œ íŒíŠ¸ë¥¼ ì£¼ì„¸ìš”.
    - "ì´ ê°œë…ì„ ë‹¤ì‹œ ê³µë¶€í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”."ë¼ê³  ì œì•ˆí•œ ë’¤, "ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‚˜ìš”?"ë¼ê³  ë¬¼ì–´ë³´ì„¸ìš”.
    - (ì˜ˆ: "ì•„ì‰½ë„¤ìš”, ì •ë‹µì€ '4'ì˜€ì–´ìš”. ğŸ˜… ì´ ë¬¸ì œë¥¼ í’€ë ¤ë©´ '+3'ì„ ë°˜ëŒ€í¸ìœ¼ë¡œ ë„˜ê¸°ëŠ” '{key_concept}' ê°œë…ì´ í•„ìš”í–ˆì–´ìš”. ì´ ê°œë…ì„ ë‹¤ì‹œ ê³µë¶€í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”. ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‚˜ìš”?")
    
í”¼ë“œë°±ì€ 2-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ, ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
"""),
        ("user", f"í•™ìƒì˜ ë‹µ: {user_answer}")
    ])
    chain = prompt | llm | StrOutputParser()
    return chain.stream({})


# 6-4. master router 
def call_master_router(user_input: str, current_state: dict) -> tuple[str, str]: # (ìˆ˜ì •) ë°˜í™˜ íƒ€ì…ì„ íŠœí”Œë¡œ ëª…ì‹œ
    """
    ì‚¬ìš©ì ì…ë ¥ê³¼ í˜„ì¬ ìƒíƒœë¥¼ ë³´ê³ , ì–´ë–¤ ì‘ì—…ìœ¼ë¡œ ë¶„ë¥˜í• ì§€ ê²°ì •í•˜ëŠ” 'êµí†µ ì •ë¦¬' LLM.
    í•­ìƒ (task, topic) 2ê°œì˜ ê°’ì„ íŠœí”Œë¡œ ë°˜í™˜
    """
    # íŠœí„° íë¦„ì— ê¹Šì´ ê´€ì—¬ëœ ìƒíƒœì¸ì§€ í™•ì¸
    mode = current_state.get("mode", "IDLE")
    
    if mode == "WAITING_PROBLEM_ANSWER":
        log_debug("ë¼ìš°í„°: ë¬¸ì œ ë‹µë³€ ëŒ€ê¸° ì¤‘ì´ë¯€ë¡œ 'solve_problem'ìœ¼ë¡œ ê°•ì œ ë¶„ë¥˜")
        return "solve_problem", "none"
    
    if mode in ["WAITING_DIAGNOSTIC", "WAITING_CONTINUATION"]:
        log_debug("ë¼ìš°í„°: íŠœí„° íë¦„(ì§„ë‹¨/ì—°ì†)ì´ ì§„í–‰ ì¤‘ì´ë¯€ë¡œ 'tutor_flow'ë¡œ ê°•ì œ ë¶„ë¥˜")
        return "tutor_flow", "none" # (ìˆ˜ì •) 2ê°œ ê°’ ë°˜í™˜

    # (ì°¸ê³ ) íê°€ ë¹„ì–´ìˆì–´ë„ POST_EXPLANATION ìƒíƒœì¼ ìˆ˜ ìˆìŒ
    queue_status = "ë¹„ì–´ìˆìŒ" if not current_state.get("queue") else "ì„¤ëª… ëŒ€ê¸° ì¤‘"
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ í•™ìƒì˜ ìš”ì²­ì„ ë¶„ë¥˜í•˜ëŠ” 'êµí†µ ì •ë¦¬' ë‹´ë‹¹ìì…ë‹ˆë‹¤.
í•™ìƒì˜ ì…ë ¥ê³¼ í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ë³´ê³ , ì´ ìš”ì²­ì„ ì–´ë–¤ ë¶€ì„œë¡œ ë³´ë‚´ì•¼ í• ì§€ ê²°ì •í•˜ì„¸ìš”.

[ë¶€ì„œ ëª©ë¡]
1.  greeting: í•™ìƒì´ ë‹¨ìˆœí•œ ì¸ì‚¬ë‚˜ ì•ˆë¶€ë¥¼ ë¬»ìŠµë‹ˆë‹¤. (ì˜ˆ: "ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš©", "ã…ã…‡")
2.  ask_problem: í•™ìƒì´ ê°œë…ì— ëŒ€í•œ 'ë¬¸ì œ'ë¥¼ í’€ì–´ë³´ê¸¸ ì›í•©ë‹ˆë‹¤. (ì˜ˆ: "ë¬¸ì œ ë‚´ì¤˜", "í€´ì¦ˆ í’€ì–´ë³¼ë˜", "ì¼ì°¨ì‹ ë¬¸ì œ í’€ì–´ë³¼ê²Œìš”")
3.  tutor_flow: í•™ìƒì´ ìˆ˜í•™ 'ê°œë…'ì„ ì§ˆë¬¸í•˜ê±°ë‚˜, ë°©ê¸ˆ ëë‚œ ê°œë… ì„¤ëª…ì— ëŒ€í•´ ì¬ì„¤ëª…/ì¶”ê°€ ì§ˆë¬¸ì„ í•©ë‹ˆë‹¤. (ê°€ì¥ ì¼ë°˜ì ì¸ ê²½ìš°)
    (ì˜ˆ: "ì¼ì°¨ë°©ì •ì‹ì´ ë­ì•¼?", "ë°©ê¸ˆ ì„¤ëª…í•œ ê±° ì´í•´ ì•ˆë¼", "ë‹¤ë¥¸ ì˜ˆì‹œ ì—†ì–´?", "xê°€ ë­”ë°?", "ë­ë¼ëŠ”ê±°ì•¼", "ë¬´ìŠ¨ ë§ì´ì•¼?")
4.  chitchat: ìˆ˜í•™ê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™” ë˜ëŠ” ê°ì‚¬ í‘œí˜„ì…ë‹ˆë‹¤. (ì˜ˆ: "ë„ˆëŠ” ëˆ„êµ¬ì•¼?", "ê³ ë§ˆì›Œ", "ìˆ˜ê³ í–ˆì–´")
5.  solve_problem: í•™ìƒì´ ë°©ê¸ˆ ì¶œì œëœ ë¬¸ì œì˜ ë‹µì„ ë§í•©ë‹ˆë‹¤. (ì˜ˆ: "3", "ì •ë‹µ 4", "x=4", "3 ì•„ë‹ˆì•¼?")

[ìƒí™©ë³„ íŠ¹ë³„ ê·œì¹™]
1. ë§Œì•½ íŠœí„°ê°€ ë°©ê¸ˆ "ë¬¸ì œ"ë¥¼ ëƒˆë‹¤ë©´ (ì˜ˆ: "ë‹µì„ ì…ë ¥í•´ì£¼ì„¸ìš”."), í•™ìƒì˜ ìˆ«ì("3"), ì •ë‹µ í™•ì¸("3 ì•„ë‹ˆì•¼?"), í’€ì´ ê³¼ì •("x=4") ë“±ì€ 'chitchat'ì´ë‚˜ 'greeting'ì´ ì•„ë‹ˆë¼, 'solve_problem'ì´ë¼ëŠ” ìƒˆ ë¶€ì„œë¡œ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤.

[í˜„ì¬ ìƒíƒœ]
- mode: {mode}
- í: {queue_status}
- ë§ˆì§€ë§‰ ì„¤ëª… ê°œë…: {current_state.get("last_explained_concept", "ì—†ìŒ")}
í•™ìƒì˜ "ë¬¸ì œ ë‚´ì¤˜"ì™€ "ë‹¤ë¥¸ ì˜ˆì‹œ"ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•´ì•¼ í•©ë‹ˆë‹¤.
- "ë¬¸ì œ ë‚´ì¤˜" -> ask_problem
- "ì¼ì°¨ì‹ ë¬¸ì œ ë‚´ì¤˜" -> ask_problem
- "ë‹¤ë¥¸ ì˜ˆì‹œ" -> tutor_flow (ì¬ì„¤ëª… ìš”ì²­ì„)

ë°˜ë“œì‹œ ë¶€ì„œ ì´ë¦„ë§Œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- `ask_problem`ì˜ ê²½ìš°, í•™ìƒì´ íŠ¹ì • ê°œë…ì„ ì–¸ê¸‰í–ˆë‹¤ë©´ "topic"ë„ ì¶”ì¶œí•˜ì„¸ìš”.
(ì˜ˆ: "ì¼ì°¨ë°©ì •ì‹ì´ ë­ì•¼" -> {{{{"task": "tutor_flow", "topic": "ì¼ì°¨ë°©ì •ì‹"}}}})
(ì˜ˆ: "3" -> {{{{"task": "solve_problem", "topic": "none"}}}})
(ì˜ˆ: "ì¼ì°¨ì‹ ë¬¸ì œ ë‚´ì¤˜" -> {{{{"task": "ask_problem", "topic": "ì¼ì°¨ì‹"}}}})
(ì˜ˆ: "ë¬¸ì œ ë‚´ì¤˜" -> {{{{"task": "ask_problem", "topic": "none"}}}})
{{{{"task": "...", "topic": "..."}}}}
"""),
        ("user", "í•™ìƒ ì…ë ¥: {input}")
    ])

    chain = prompt | llm | StrOutputParser()
    result_str = chain.invoke({
        "input": user_input
    }).strip()

    try:
        data = json.loads(result_str)
        task = data.get("task", "tutor_flow")
        topic = data.get("topic", "none") # (ìˆ˜ì •) topic ì¶”ì¶œ
        
        # IDLE ìƒíƒœì¸ë° "ê°œë…ì—†ìŒ" ì˜¤ë¥˜ê°€ ë‚  ë§Œí•œ ì…ë ¥ì€ chitchatìœ¼ë¡œ ìœ ë„
        if task == "tutor_flow" and mode == "IDLE":
            concept = extract_concept(user_input)
            if concept == "ê°œë…ì—†ìŒ":
                # "ê°œë…ì—†ìŒ"ì¼ ë•Œë§Œ ì§§ì€ ì…ë ¥ì„ greeting/chitchatìœ¼ë¡œ ë³€ê²½
                if len(user_input.split()) < 4 and len(user_input) < 15:
                    log_debug("ë¼ìš°í„°: 'tutor_flow'ì˜€ìœ¼ë‚˜ 'ê°œë…ì—†ìŒ'ì´ ì˜ˆìƒë˜ì–´ 'greeting'ìœ¼ë¡œ ë³€ê²½")
                    return "greeting", "none" # (ìˆ˜ì •) 2ê°œ ê°’ ë°˜í™˜
                else:
                    log_debug("ë¼ìš°í„°: 'tutor_flow'ì˜€ìœ¼ë‚˜ 'ê°œë…ì—†ìŒ'ì´ ì˜ˆìƒë˜ì–´ 'chitchat'ìœ¼ë¡œ ë³€ê²½")
                    return "chitchat", "none" # (ìˆ˜ì •) 2ê°œ ê°’ ë°˜í™˜
            
            # ê°œë…ì´ ìˆìœ¼ë©´(else) taskì™€ topicì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            log_debug(f"ë¼ìš°í„°: 'tutor_flow' (IDLE)ë¡œ ë¶„ë¥˜, topic: {topic}")
            return task, topic 
            
        log_debug(f"ë¼ìš°í„°: '{task}' (Non-IDLE)ë¡œ ë¶„ë¥˜, topic: {topic}")
        return task, topic # IDLEì´ ì•„ë‹Œ ê²½ìš° (ì›ë˜ 2ê°œ ê°’ ë°˜í™˜í•˜ë˜ ê³³)
    
    except Exception as e:
        print(f"âš ï¸ ë§ˆìŠ¤í„° ë¼ìš°í„° JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return "tutor_flow", "none" # 2ê°œ ê°’ ë°˜í™˜

# 6-5. LLM ì˜ë„ ë¶„ë¥˜ê¸° (tutor_flow ë‚´ë¶€ì—ì„œë§Œ ì‚¬ìš©ë¨) 
def classify_continuation_intent(user_response: str, next_concept: str = None, question_type: str = "shall_i_explain", last_explained_concept: str = "none") -> dict:
    """
    (tutor_flow ì „ìš©) í•™ìƒì˜ ë‹µë³€ ì˜ë„ë¥¼ LLMì„ í†µí•´ ë¶„ë¥˜
    """
    if question_type == 'do_you_know':
        tutor_question_context = f"íŠœí„°ê°€ ë°©ê¸ˆ '{next_concept}'(ì€)ëŠ” ì•Œê³  ê³„ì‹ ì§€ ë¬¼ì–´ë´¤ìŠµë‹ˆë‹¤."
        intent_list_intro = "í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì˜ë„ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:"
        intent_list = f"""
1.  "continue": '{next_concept}' ì„¤ëª…ì„ ë“£ê¸¸ ì›í•¨.
    - **ë§¤ìš° ì¤‘ìš”:** "ë„¤", "ì‘", "**ì›…**", "ë§ì•„ìš”" ë“± **ë‹¨ í•œ ë‹¨ì–´ë¡œ ëœ ê¸ì • ë‹µë³€**ì€ **ì ˆëŒ€ë¡œ** ë‹¤ë¥¸ ì˜ë„ë¡œ ë¶„ë¥˜í•˜ì§€ ë§ê³  **ë¬´ì¡°ê±´ "continue"**ë¡œ ë¶„ë¥˜í•´ì•¼ í•©ë‹ˆë‹¤.
    - ì„¤ëª…ì„ ì§ì ‘ ìš”ì²­í•˜ëŠ” ê²½ìš° (ì˜ˆ: "ì„¤ëª…í•´ì¤˜", "ì•Œë ¤ì¤˜", "ê·¸ê²Œ ë­”ë°?")
2.  "skip": '{next_concept}' ì„¤ëª…ì„ ê±´ë„ˆë›°ê¸¸ ì›í•¨ (ì´ë¯¸ ì•ˆë‹¤ê³  ë‹µí•¨). (ì˜ˆ: "ì•Œì•„ìš”", "ê´œì°®ì•„ìš”", "ëì–´")
3.  "re-explain": **ë°©ê¸ˆ ì„¤ëª…í•œ ê°œë…({last_explained_concept})**ì— ëŒ€í•œ ì¬ì„¤ëª…/ì¶”ê°€ ì„¤ëª… ìš”ì²­.
    - (ì˜ˆ: "ì•„ì§ ì´í•´ì•ˆë¼", "ì˜ ëª¨ë¥´ê² ì–´", "ë­ë”ë¼", "ë°©ê¸ˆ ê·¸ê²Œ ë¬´ìŠ¨ ë§ì´ì•¼?", "ëª¨ë¥´ê² ì–´")
    - (ì˜ˆ: "ì•„ë‹ˆ {last_explained_concept}(ì´)ê°€ ì´í•´ ì•ˆë¼", "ì•„ë‹ˆ {last_explained_concept}(ì„)ë¥¼ ëª¨ë¥´ê² ë‹¤ê³ ")
4.  "new_question": '{next_concept}'ê³¼ ë¬´ê´€í•œ ìƒˆ ì§ˆë¬¸.
5.  "unclear": ìœ„ ì–´ë””ì—ë„ í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë¶ˆëª…í™•í•œ ë‹µë³€. (ì˜ˆ: "ì•„ë‹ˆ", "ì‘?", "ìŒ...")
"""
    elif question_type == 'shall_i_explain':
        tutor_question_context = f"íŠœí„°ê°€ ë°©ê¸ˆ '{next_concept}'(ì„)ë¥¼ ì„¤ëª…í•´ì¤„ì§€ ë¬¼ì–´ë´¤ìŠµë‹ˆë‹¤."
        intent_list_intro = "í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì˜ë„ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:"
        intent_list = f"""
1.  "continue": '{next_concept}' ì„¤ëª…ì„ ë“£ê¸¸ ì›í•¨.
    - **ë§¤ìš° ì¤‘ìš”:** "ë„¤", "ì‘", "**ì›…**", "ë§ì•„ìš”" ë“± **ë‹¨ í•œ ë‹¨ì–´ë¡œ ëœ ê¸ì • ë‹µë³€**ì€ **ì ˆëŒ€ë¡œ** ë‹¤ë¥¸ ì˜ë„ë¡œ ë¶„ë¥˜í•˜ì§€ ë§ê³  **ë¬´ì¡°ê±´ "continue"**ë¡œ ë¶„ë¥˜í•´ì•¼ í•©ë‹ˆë‹¤.
    - ì„¤ëª…ì„ ì§ì ‘ ìš”ì²­í•˜ëŠ” ê²½ìš° (ì˜ˆ: "ì„¤ëª…í•´ì¤˜", "ì•Œë ¤ì¤˜", "ê·¸ê²Œ ë­”ë°?")
2.  "skip": '{next_concept}' ì„¤ëª…ì„ ê±´ë„ˆë›°ê¸¸ ì›í•¨ (ì´ë¯¸ ì•ˆë‹¤ê³  ë‹µí•¨). (ì˜ˆ: "ì•Œì•„ìš”", "ê´œì°®ì•„ìš”", "ëì–´")
3.  "re-explain": **ë°©ê¸ˆ ì„¤ëª…í•œ ê°œë…({last_explained_concept})**ì— ëŒ€í•œ ì¬ì„¤ëª…/ì¶”ê°€ ì„¤ëª… ìš”ì²­.
    - (ì˜ˆ: "ì•„ì§ ì´í•´ì•ˆë¼", "ì˜ ëª¨ë¥´ê² ì–´", "ë­ë”ë¼", "ë°©ê¸ˆ ê·¸ê²Œ ë¬´ìŠ¨ ë§ì´ì•¼?", "ëª¨ë¥´ê² ì–´")
    - (ì˜ˆ: "ì•„ë‹ˆ {last_explained_concept}(ì´)ê°€ ì´í•´ ì•ˆë¼", "ì•„ë‹ˆ {last_explained_concept}(ì„)ë¥¼ ëª¨ë¥´ê² ë‹¤ê³ ")
4.  "new_question": '{next_concept}'ê³¼ ë¬´ê´€í•œ ìƒˆ ì§ˆë¬¸.
5.  "unclear": ìœ„ ì–´ë””ì—ë„ í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë¶ˆëª…í™•í•œ ë‹µë³€. (ì˜ˆ: "ì•„ë‹ˆ", "ì‘?", "ìŒ...")
"""
    else: # post_explanation (ì´ ë¶€ë¶„ì€ ë¼ìš°í„°ê°€ ì²˜ë¦¬í•¨. ìˆ˜ì • ìš”)
        tutor_question_context = "íŠœí„°ê°€ ë°©ê¸ˆ ê°œë… ì„¤ëª…ì„ ë§ˆì¹˜ê³  'ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆë‚˜ìš”?'ë¼ê³  ë¬¼ì—ˆìŠµë‹ˆë‹¤."
        intent_list_intro = "í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì˜ë„ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:"
        intent_list = """
1.  "re-explain": ë°©ê¸ˆ ì„¤ëª…ë“¤ì€ ê°œë…ì— ëŒ€í•œ ì¬ì„¤ëª…/ì¶”ê°€ ì„¤ëª… ìš”ì²­ (ì˜ˆ: "ì•„ì§ ì´í•´ì•ˆë¼", "ë‹¤ë¥¸ ì˜ˆì‹œ ì—†ì–´?", "ì¢€ ë” ì„¤ëª…í•´ì¤˜").
2.  "new_question": ìƒˆë¡œìš´ ìˆ˜í•™ ì§ˆë¬¸ (ë¬¸ì œê°€ ì•„ë‹Œ ê°œë… ì§ˆë¬¸).
3.  "acknowledged": ì„¤ëª…ì„ ì˜ ë“¤ì—ˆë‹¤ëŠ” ë‹¨ìˆœ ê¸ì •/ê°ì‚¬ í‘œí˜„ (ì˜ˆ: "ë„¤", "ì›…", "ì•Œê² ìŠµë‹ˆë‹¤", "ê³ ë§ˆì›Œìš”").
4.  "unclear": ì˜ë„ê°€ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ìˆ˜í•™ê³¼ ê´€ë ¨ ì—†ëŠ” ëŒ€í™”.
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ í•™ìƒì˜ ë‹µë³€ ì˜ë„ë¥¼ ë§¤ìš° ì •í™•í•˜ê²Œ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
{tutor_question_context}

{intent_list_intro}
{intent_list}

**ë¶€ê°€ì ì¸ ì§ˆë¬¸(clarification_question):**
- í•™ìƒì´ ì£¼ëœ ì˜ë„ì™€ **ë³„ê°œë¡œ**, ì¶”ê°€ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. ì—†ìœ¼ë©´ nullì…ë‹ˆë‹¤.
- (ì˜ˆ: "ì›… ê·¼ë° ê·¸ê±°ë‘ ì¢Œí‘œí‰ë©´ì´ë‘ ë­”ìƒê´€ì´ì§€" -> "ìˆœì„œìŒê³¼ ì¢Œí‘œí‰ë©´ì˜ ì—°ê´€ì„±ì— ëŒ€í•œ ì§ˆë¬¸")

**ì¶”ê°€ ì •ë³´ (topic):**
- ì£¼ëœ ì˜ë„ê°€ "re-explain" ë˜ëŠ” "new_question"ì¼ ê²½ìš°, ê´€ë ¨ëœ ìˆ˜í•™ ê°œë…(topic)ì„ ì¶”ì¶œí•˜ì„¸ìš”.
- **ë§¤ìš° ì¤‘ìš”:** "re-explain" ì˜ë„ì¼ ë•Œ, í•™ìƒì´ **"A"ë¥¼ ì„¤ëª…í•´ë‹¬ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ë§í–ˆë‹¤ë©´ (ì˜ˆ: "Aê°€ ë­ë”ë¼", "A ë‹¤ì‹œ ì„¤ëª…í•´ì¤˜", "Aê°€ ì´í•´ ì•ˆë¼", "Aê°€ ë­”ë°")**,
  íŠœí„°ê°€ ë°©ê¸ˆ Bì— ëŒ€í•´ ë¬¼ì–´ë´¤ë”ë¼ë„ **ë°˜ë“œì‹œ "A"ë¥¼ topicìœ¼ë¡œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.** (ì˜ˆ: íŠœí„°ê°€ 'ì¼ì°¨ì‹'ì„ ë¬¼ì—ˆì–´ë„ í•™ìƒì´ 'ë°©ì •ì‹ ì„¤ëª…í•´ì¤˜'ë¼ê³  í•˜ë©´ topicì€ 'ë°©ì •ì‹'ì…ë‹ˆë‹¤.)
- í•™ìƒì´ ëª…ì‹œì ìœ¼ë¡œ topicì„ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´ (ì˜ˆ: "ë‹¤ì‹œ ì„¤ëª…í•´ì¤˜", "ì´í•´ ì•ˆë¼"),
  "re-explain" ì˜ë„ì¼ ê²½ìš° topicì„ **"{last_explained_concept}"**(ìœ¼)ë¡œ ì„¤ì •í•˜ì„¸ìš”.
  "new_question" ì˜ë„ì¼ ê²½ìš° "none"ì„ ë°˜í™˜í•˜ì„¸ìš”.
  
**ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON):**
{{{{"primary_intent": "...", "clarification_question": "...", "topic": "..."}}}}
(clarification_questionì´ ì—†ìœ¼ë©´ null, topicì€ í•´ë‹¹ ì—†ì„ ì‹œ "none")
"""),
        ("user", "í•™ìƒ ë‹µë³€: {response}")
    ])

    chain = prompt | llm | StrOutputParser()
    result_str = chain.invoke({
        "response": user_response,
        "last_explained_concept": last_explained_concept # ì´ ì¤„ ì¶”ê°€
    }).strip()

    try:
        data = json.loads(result_str)
        data.setdefault("primary_intent", "unclear")
        data.setdefault("clarification_question", None)
        data.setdefault("topic", "none")
        return data
    except Exception as e:
        print(f"âš ï¸ ì˜ë„ ë¶„ë¥˜ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {"primary_intent": "unclear", "clarification_question": None, "topic": "none"}
        
# 7. ë©”ì¸ íŠœí„° ë¡œì§ (tutor_flow ì „ìš©)
def intelligent_tutor(user_question: str, explained_concepts: set, explanation_count: dict) -> dict:
    """ì „ì²´ íŠœí„°ë§ í”„ë¡œì„¸ìŠ¤ (ê¸°ì–µë ¥ + ì„¤ëª… íšŸìˆ˜ + Fallback ì¶”ê°€)"""
    log_debug(f"intelligent_tutor í˜¸ì¶œ: ì§ˆë¬¸='{user_question}', ê¸°ì–µ={explained_concepts}, íšŸìˆ˜={explanation_count}")
    print(f"\n{'='*50}")
    print(f"ğŸ“š ì§ˆë¬¸: {user_question}")
    print(f"{'='*50}\n")
    
    # 1) ê°œë… ì¶”ì¶œ
    concept = extract_concept(user_question)
    print(f"ğŸ” ì¶”ì¶œëœ ê°œë…: {concept}\n")
    
    if concept == "ê°œë…ì—†ìŒ":
        # ì´ ë¡œì§ì€ ì´ì œ ë¼ìš°í„°ì˜ ì•ˆì „ì¥ì¹˜ì— ì˜í•´ ê±°ì˜ í˜¸ì¶œë˜ì§€ ì•ŠìŒ
        return {"error": "ì§ˆë¬¸ì—ì„œ ìˆ˜í•™ ê°œë…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    # 2) ê°œë… ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìˆ˜ì •: None ì²˜ë¦¬ ì¶”ê°€)
    concept_info = retrieve_concept_from_graph(concept)
    
    if not concept_info:
        print(f"â„¹ï¸ '{concept}' ê°œë…ì„ ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ â†’ LLM Fallback ì‹œë„\n")
        log_missing_concept(concept)
        return {"fallback_needed": True, "concept": concept, "learning_path": {"nodes": [], "edges": []}}

    path_data = get_path_for_visualization(concept)
    
    # 3) ì„ ìˆ˜ ê°œë… ì°¾ê¸° (ê·¸ë˜í”„ì— ê°œë…ì´ ìˆëŠ” ê²½ìš°)
    all_prerequisites = get_prerequisites(concept)
    
    if not all_prerequisites:
        print("â„¹ï¸ ì„ ìˆ˜ ê°œë… ì—†ìŒ â†’ ë°”ë¡œ ì„¤ëª…\n")
        count = explanation_count.get(concept, 0)
        explanation_stream = generate_explanation(concept_info, count)
        return {
            "concept": concept,
            "explanation_stream": explanation_stream,
            "needs_diagnosis": False,
            "learning_path": path_data
        }


    immediate_prereqs = [p for p in all_prerequisites if p['depth'] == 1]
    prereqs_to_check = [p for p in immediate_prereqs if p['name'] not in explained_concepts]

    if not prereqs_to_check:
        print(f"â„¹ï¸ ì„ ìˆ˜ ê°œë… ({[p['name'] for p in immediate_prereqs]}) (ì´ë¯¸ í•™ìŠµë¨) â†’ ë°”ë¡œ ì„¤ëª…\n")
        count = explanation_count.get(concept, 0)
        explanation_stream = generate_explanation(concept_info, count)
        return {
            "concept": concept, "concept_info": concept_info,
            "prerequisites": [], "needs_diagnosis": False,
            "explanation_stream": explanation_stream,
            "learning_path": path_data
        }

    print(f"ğŸ“‹ í™•ì¸ í•„ìš”í•œ ì„ ìˆ˜ ê°œë…: {[p['name'] for p in prereqs_to_check]}\n")
    diagnostic_q_stream = generate_diagnostic_question(concept, prereqs_to_check)
    
    return {
        "concept": concept,
        "concept_info": concept_info,
        "prerequisites": prereqs_to_check,
        "needs_diagnosis": True,
        "diagnostic_question_stream": diagnostic_q_stream,
        "learning_path": path_data
    }

# 7-1. ì§„ë‹¨ ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜ (tutor_flow ì „ìš©)
def handle_diagnostic_response(concept_info: dict, user_response: str, prerequisites: list, explanation_count: dict) -> dict:
    """
    ì§„ë‹¨ ì§ˆë¬¸ì— ëŒ€í•œ í•™ìƒ ë‹µë³€ì„ ì²˜ë¦¬í•˜ê³ , ì„¤ëª… íë¥¼ ìƒì„±í•˜ì—¬ ì²« ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ’¬ í•™ìƒ ë‹µë³€: {user_response}\n")
    
    prereq_names = [p["name"] for p in prerequisites]
    
    # 1) ì´í•´ë„ íŒë‹¨
    understanding_map = assess_understanding(user_response, prereq_names)
    print(f"ğŸ“Š ì´í•´ë„ ë¶„ì„: {understanding_map}\n")
    
    # 2) ì„¤ëª… í ìƒì„± 
    explanation_queue, unmentioned_concepts = build_explanation_queue(
        understanding_map, concept_info['name']
    )
    
    concept_to_explain_name = explanation_queue.pop(0) 
    
    current_concept_info = None
    if concept_to_explain_name == concept_info['name']:
        current_concept_info = concept_info
    else:
        current_concept_info = retrieve_concept_from_graph(concept_to_explain_name)

    count = explanation_count.get(concept_to_explain_name, 0)
    
    if not current_concept_info:
        first_explanation_text = f"'{concept_to_explain_name}' ê°œë…ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        explanation_stream = iter([first_explanation_text])
    else:
        explanation_stream = generate_explanation(current_concept_info, count)
    
    # 3) í›„ì† ì§ˆë¬¸ ìƒì„± (ìŠ¤íŠ¸ë¦¼ì— ì¶”ê°€í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í•„ìš”)
    follow_up_text = ""
    if explanation_queue:
        next_concept_name = explanation_queue[0]
        is_next_unmentioned = next_concept_name in unmentioned_concepts 
        if is_next_unmentioned:
            follow_up_text = f"\n\nğŸ’¡ ì´ ê°œë…ì´ ì´í•´ë˜ì…¨ë‚˜ìš”? ê·¸ëŸ¼ '{next_concept_name}'(ì€)ëŠ” ì•Œê³  ê³„ì‹ ê°€ìš”?"
        else:
            follow_up_text = f"\n\nğŸ’¡ ì´ ê°œë…ì´ ì´í•´ë˜ì…¨ë‚˜ìš”? ë‹¤ìŒìœ¼ë¡œ '{next_concept_name}'(ì„)ë¥¼ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?"
    else:
        follow_up_text = f"\n\nğŸ’¡ '{concept_to_explain_name}'ì— ëŒ€í•œ ì„¤ëª…ì´ ëë‚¬ì–´ìš”. ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆë‚˜ìš”?"

    return {
        "explanation_stream": explanation_stream,
        "follow_up_text": follow_up_text,
        "queue": explanation_queue,
        "understanding_map": understanding_map,
        "unmentioned_concepts": unmentioned_concepts,
        "explained_concept_name": concept_to_explain_name
    }

# 7-2. ì„¤ëª… í ìƒì„± í•¨ìˆ˜ (tutor_flow ì „ìš©)
def build_explanation_queue(understanding_map: dict, target_concept: str) -> tuple[list, list]:
    """
    ì´í•´ë„ ë§µìœ¼ë¡œë¶€í„° ì„¤ëª… í(Queue)ì™€ ì–¸ê¸‰ ì•ˆ ëœ ê°œë… ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.
    í ìˆœì„œ: ëª¨ë¥´ëŠ” ê²ƒ(False) -> ì–¸ê¸‰ ì•ˆ í•œ ê²ƒ(None) -> ëª©í‘œ ê°œë…
    """
    unknown = [name for name, status in understanding_map.items() if status is False]
    unmentioned = [name for name, status in understanding_map.items() if status is None]
    known = [name for name, status in understanding_map.items() if status is True]
    
    queue = unknown + unmentioned + [target_concept]
    
    seen = set(known) 
    unique_queue = []
    for concept in queue:
        if concept not in seen:
            unique_queue.append(concept)
            seen.add(concept)
            
    final_queue = unique_queue or [target_concept] 
    
    print(f"ğŸ§  ì„¤ëª… í ìƒì„±: {final_queue} (ë‹¤ìŒ ëŒ€ê¸°)")
    return final_queue, unmentioned

# 8. ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ê°ì§€ í•¨ìˆ˜
def is_system_command(text: str) -> bool:
    """ì…ë ¥ì´ ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ë˜ëŠ” ì½”ë“œ ì¡°ê°ì¸ì§€ ê°„ë‹¨íˆ ê°ì§€í•©ë‹ˆë‹¤."""
    if not text:
        return False
        
    FILTER_KEYWORDS = ["pyenv", "python", ".py", "conda", "pip", "import ", "def ", "class "]
    text_lower = text.lower()
    
    if len(text.split()) < 5 and any(kw in text_lower for kw in FILTER_KEYWORDS):
        return True
        
    if "/" in text or "\\" in text:
        if text.replace("/", "").replace(" ", "").isdigit():
             return False
        return True

    return False

# 8-1. ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜
def reset_conversation_flow(state: dict, keep_memory: bool = True):
    """
    ëŒ€í™” íë¦„ ê´€ë ¨ ìƒíƒœ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    keep_memoryê°€ Falseì´ë©´ í•™ìŠµ ê¸°ì–µ(explained_concepts ë“±)ê¹Œì§€ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    print("ğŸ”„ ëŒ€í™” íë¦„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
    state["mode"] = "IDLE"
    state["queue"] = []
    state["unmentioned_concepts"] = []
    state["last_tutor_question_type"] = None
    state["target_concept_info"] = None
    state["prerequisites"] = []
    state["primary_goal_concept"] = None
    state["pending_input"] = None 
    state["learning_path"] = {"nodes": [], "edges": []} # <-- ì´ ì¤„ ì¶”ê°€

    if not keep_memory:
        print("ğŸ§  í•™ìŠµ ê¸°ì–µë„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        state["explained_concepts"] = set()
        state["explanation_count"] = {}
        state["last_explained_concept"] = None
    
# 8-3. ëˆ„ë½ ê°œë… ê¸°ë¡ í•¨ìˆ˜
def log_missing_concept(concept_name: str, log_file="missing_concepts.log"):
    """ê·¸ë˜í”„ì— ì—†ëŠ” ê°œë…ì„ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"{concept_name}\n")
        log_debug(f"'{concept_name}' ê°œë… ëˆ„ë½ ê¸°ë¡ ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ ëˆ„ë½ ê°œë… ë¡œê¹… ì˜¤ë¥˜: {e}")


# 9. í•µì‹¬ íŠœí„° ìƒíƒœ ë¨¸ì‹  í•¨ìˆ˜
def handle_tutor_flow(user_input: str, new_state: dict) -> dict:
    """
    ë³µì¡í•œ íŠœí„°ë§ ìƒíƒœ ë¨¸ì‹  (State Machine) ë¡œì§.
    ì˜¤ì§ 'ìˆ˜í•™ ê°œë… ì„¤ëª…'ì˜ íë¦„ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    (ìˆ˜ì •) prefix, stream, textë¥¼ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    log_debug(f"í•µì‹¬ íŠœí„° ë¡œì§(handle_tutor_flow) ì‹¤í–‰...")
    
    # ìŠ¤íŠ¸ë¦¼ ë°˜í™˜ìš© ë³€ìˆ˜
    response_stream = None
    response_text = ""
    response_prefix = "" 

    current_mode = new_state.get("mode", "IDLE")

    # --- ìƒíƒœ 0: ì„¤ëª… ì™„ë£Œ í›„ ---
    if current_mode == "POST_EXPLANATION":
        intent_data = classify_continuation_intent(user_input, question_type="post_explanation", last_explained_concept=new_state.get("last_explained_concept", "none"))
        primary_intent = intent_data.get("primary_intent")
        topic = intent_data.get("topic")
        log_debug(f"POST_EXPLANATION(tutor_flow) ì˜ë„ ë¶„ì„: ì£¼={primary_intent}, ì£¼ì œ={topic}")

        if primary_intent == "re-explain":
            input_for_next_turn = ""
            if topic != "none":
                input_for_next_turn = topic
            else:
                if user_input not in ["??", "?", "í ", "ìŒ"]:
                     input_for_next_turn = user_input
                else:
                     input_for_next_turn = new_state.get("last_explained_concept")

            if input_for_next_turn:
                response_text = f"ğŸ”„ ('{input_for_next_turn}'(ìœ¼)ë¡œ ì¬ì„¤ëª… ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.)"
                reset_conversation_flow(new_state)
                new_state["pending_input"] = input_for_next_turn 
            else:
                response_text = "ì–´ë–¤ ê°œë…ì„ ë‹¤ì‹œ ì„¤ëª…í•´ ë“œë¦´ê¹Œìš”?"
                new_state["mode"] = "POST_EXPLANATION"
            
        elif primary_intent == "new_question":
            response_text = "ğŸ”„ (ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.)"
            reset_conversation_flow(new_state)
            new_state["pending_input"] = topic if topic != "none" else user_input

        elif primary_intent == "acknowledged":
            response_text = "ë„¤! ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ í¸í•˜ê²Œ í•´ì£¼ì„¸ìš”."
            new_state["mode"] = "POST_EXPLANATION"
        
        else: # unclear
            response_text = "í ... ë°©ê¸ˆ í•˜ì‹  ë§ì”€ì´ ì¬ì„¤ëª… ìš”ì²­ì¸ì§€, ìƒˆ ì§ˆë¬¸ì¸ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            new_state["mode"] = "POST_EXPLANATION"

    # --- ìƒíƒœ 1: "ë‹¤ìŒ ì„¤ëª…í•´ì¤„ê¹Œ?" / "ì•Œê³  ê³„ì‹ ê°€ìš”?" ë‹µë³€ ì²˜ë¦¬ ---
    elif current_mode == "WAITING_CONTINUATION":
        current_queue = new_state.get("queue", [])
        if not current_queue:
            log_debug("WAITING_CONTINUATION ìƒíƒœì¸ë° íê°€ ë¹„ì–´ìˆìŒ - ì´ˆê¸°í™”")
            reset_conversation_flow(new_state)
            response_text = "í , ëŒ€í™”ê°€ ì ì‹œ ê¼¬ì˜€ë„¤ìš”. ìƒˆë¡œìš´ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
            # (ìˆ˜ì •) prefix í‚¤ ì¶”ê°€
            return {"response_prefix": "", "response_text": response_text, "response_stream": None, "new_state": new_state}
        
        next_concept = current_queue[0]
        question_type = new_state.get("last_tutor_question_type", "shall_i_explain")

        intent_data = classify_continuation_intent(user_input, next_concept, question_type, last_explained_concept=new_state.get("last_explained_concept", "none"))
        primary_intent = intent_data.get("primary_intent")
        clarification_q = intent_data.get("clarification_question")
        topic = intent_data.get("topic")
        log_debug(f"WAITING_CONTINUATION ì˜ë„ ë¶„ì„: ì£¼={primary_intent}, ë¶€ê°€ì§ˆë¬¸={clarification_q}, ì£¼ì œ={topic}")

        explanation_stream = None
        follow_up_text = ""

        if clarification_q:
            last_concept = new_state.get("last_explained_concept", "ì´ì „ ê°œë…")
            response_prefix = f"ì¢‹ì€ ì§ˆë¬¸ì´ì—ìš”! '{last_concept}'(ì€)ëŠ” '{next_concept}'(ì„)ë¥¼ ì´í•´í•˜ëŠ” ë° ê¼­ í•„ìš”í•œ ê¸°ì´ˆ ê°œë…ì´ëë‹ˆë‹¤. "
            log_debug(f"ë¶€ê°€ ì§ˆë¬¸ ë‹µë³€(ì ‘ë‘ì‚¬) ìƒì„±: {response_prefix}")

        if primary_intent == "continue":
            concept_to_explain_name = current_queue.pop(0)
            log_debug(f"ì„¤ëª… ì§„í–‰: {concept_to_explain_name}")
            current_concept_info = None
            
            if concept_to_explain_name == new_state["target_concept_info"]["name"]:
                 current_concept_info = new_state["target_concept_info"]
            else:
                 current_concept_info = retrieve_concept_from_graph(concept_to_explain_name)
            
            count = new_state["explanation_count"].get(concept_to_explain_name, 0)
            
            if not current_concept_info:
                 explanation_stream = iter([f"'{concept_to_explain_name}' ê°œë…ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."])
            else:
                 explanation_stream = generate_explanation(current_concept_info, count)
            
            new_state["explained_concepts"].add(concept_to_explain_name)
            new_state["explanation_count"][concept_to_explain_name] = count + 1
            new_state["last_explained_concept"] = concept_to_explain_name

        elif primary_intent == "skip":
            skipped_concept = current_queue.pop(0)
            log_debug(f"ì„¤ëª… ê±´ë„ˆë›°ê¸°: {skipped_concept}")
            explanation_stream = iter([f"ì•Œê² ìŠµë‹ˆë‹¤! '{skipped_concept}'(ì€)ëŠ” ì´ë¯¸ ì•Œê³  ê³„ì…¨êµ°ìš”."])
            new_state["explained_concepts"].add(skipped_concept)

        elif primary_intent == "re-explain":
            log_debug(f"{topic} ì¬ì„¤ëª… ìš”ì²­")
            r_info = retrieve_concept_from_graph(topic)
            count = new_state["explanation_count"].get(topic, 0)
            
            if r_info:
                 response_prefix += f"ì•„, '{topic}'(ì´)ê°€ ì•„ì§ ì´í•´ê°€ ì•ˆ ë˜ì…¨êµ°ìš”. ë‹¤ì‹œ ì„¤ëª…í•´ë“œë¦´ê²Œìš”.\n\n"
                 explanation_stream = generate_explanation(r_info, count)
            else:
                 explanation_stream = iter([f"'{topic}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë„¤ìš”."])
            
            new_state["explained_concepts"].add(topic)
            new_state["explanation_count"][topic] = count + 1
            new_state["last_explained_concept"] = topic

        else: # new_question or unclear
            if primary_intent == "new_question" and topic != "none":
                 log_debug(f"ìƒˆ ì§ˆë¬¸ ê°ì§€: {topic}")
                 response_text = f"ì•Œê² ìŠµë‹ˆë‹¤! ê·¸ëŸ¼ '{topic}'ì— ëŒ€í•´ ë¨¼ì € ì•Œì•„ë³¼ê¹Œìš”?"
                 reset_conversation_flow(new_state)
                 new_state["pending_input"] = topic
            else: 
                 # (ìˆ˜ì •) ì˜ë„ ë¶ˆëª…í™• ì‹œ, ì´ˆê¸°í™” ëŒ€ì‹  ì¬ì§ˆë¬¸
                 log_debug(f"ì˜ë„ ë¶ˆëª…í™•({primary_intent}) - ì¬ì§ˆë¬¸ ì‹œë„")
                 response_text = f"ì£„ì†¡í•´ìš”, '{user_input}'(ì´)ë¼ê³  í•˜ì‹  ê²Œ 'ë„¤'ë¼ëŠ” ëœ»ì¸ì§€ 'ì•„ë‹ˆì˜¤'ë¼ëŠ” ëœ»ì¸ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš”. '{next_concept}' ê°œë…ì„ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?"
                 
                 # (ìˆ˜ì •) í˜„ì¬ ìƒíƒœì™€ íë¥¼ ìœ ì§€í•œ ì±„, ì§ˆë¬¸ ë°©ì‹ë§Œ ë³€ê²½
                 new_state["mode"] = "WAITING_CONTINUATION"
                 new_state["last_tutor_question_type"] = "shall_i_explain" # ì§ˆë¬¸ì„ "ì„¤ëª…í•´ì¤„ê¹Œìš”?"ë¡œ ëª…í™•í•˜ê²Œ
           
            return {"response_prefix": response_prefix, "response_text": response_text, "response_stream": None, "new_state": new_state}
      
        #í›„ì† ì²˜ë¦¬
        if current_queue:
            next_concept_name = current_queue[0]
            is_next_unmentioned = next_concept_name in new_state.get("unmentioned_concepts", [])
            
            if is_next_unmentioned:
                q_type = "do_you_know"
                response_text = f"\n\nğŸ’¡ ê·¸ëŸ¼ ë‹¤ìŒìœ¼ë¡œ '{next_concept_name}'(ì€)ëŠ” ì•Œê³  ê³„ì‹ ê°€ìš”?"
            else:
                q_type = "shall_i_explain"
                response_text = f"\n\nğŸ’¡ ì´ ê°œë…ì´ ì´í•´ë˜ì…¨ë‚˜ìš”? ë‹¤ìŒìœ¼ë¡œ '{next_concept_name}'(ì„)ë¥¼ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?"
            
            new_state["mode"] = "WAITING_CONTINUATION"
            new_state["queue"] = current_queue
            new_state["last_tutor_question_type"] = q_type
        else:
            response_text = f"\n\nğŸ’¡ ëª¨ë“  ì„¤ëª…ì´ ëë‚¬ì–´ìš”! ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆë‚˜ìš”?"
            new_state["mode"] = "POST_EXPLANATION"

        response_stream = explanation_stream

    # --- ìƒíƒœ 2: "ë°©ì •ì‹ ì•Œì•„?"ì— ëŒ€í•œ ë‹µë³€ ì²˜ë¦¬ ---
    elif current_mode == "WAITING_DIAGNOSTIC":
        # (ìˆ˜ì •) ì§„ë‹¨ ë‹µë³€ë„ ì˜ë„ ë¶„ë¥˜ë¥¼ ë¨¼ì € ìˆ˜í–‰ (ìƒˆ ì§ˆë¬¸/ì¬ì„¤ëª… ë“± ì¤‘ë‹¨ ìš”ì²­ ê°ì§€)
        prereq_names = [p["name"] for p in new_state.get("prerequisites", [])]
        intent_data = classify_continuation_intent(user_input, 
                                                   next_concept=", ".join(prereq_names), 
                                                   question_type="do_you_know", 
                                                   last_explained_concept=new_state.get("last_explained_concept", "none"))
        
        primary_intent = intent_data.get("primary_intent")
        topic = intent_data.get("topic")
        log_debug(f"WAITING_DIAGNOSTIC ì˜ë„ ë¶„ì„: ì£¼={primary_intent}, ì£¼ì œ={topic}")

        if primary_intent == "new_question":
             log_debug(f"ì§„ë‹¨ ì¤‘ ìƒˆ ì§ˆë¬¸ ê°ì§€: {topic}")
             response_text = f"ì•Œê² ìŠµë‹ˆë‹¤! ê·¸ëŸ¼ '{topic}'ì— ëŒ€í•´ ë¨¼ì € ì•Œì•„ë³¼ê¹Œìš”?"
             reset_conversation_flow(new_state)
             new_state["pending_input"] = topic if topic != "none" else user_input
             return {"response_prefix": "", "response_text": response_text, "response_stream": None, "new_state": new_state}
        
        # "continue", "skip", "unclear" (ê¸°ì¡´ ë‹µë³€)ì¼ ë•Œë§Œ ì§„ë‹¨ ì‘ë‹µ ì²˜ë¦¬
        # "re-explain"ì€ ì´ ìƒíƒœì—ì„œ "unclear"ë¡œ ì²˜ë¦¬ë˜ì–´ë„ ë¬´ë°©
        result = handle_diagnostic_response(
            new_state["target_concept_info"],
            user_input,
            new_state["prerequisites"],
            new_state["explanation_count"]
        )
        response_stream = result['explanation_stream']
        response_text = result.get('follow_up_text', '') 
        understanding_map = result.get("understanding_map", {})
        for name, understood in understanding_map.items():
            if understood is True:
                new_state["explained_concepts"].add(name)
        
        explained_name = result["explained_concept_name"]
        new_state["explained_concepts"].add(explained_name)
        count = new_state["explanation_count"].get(explained_name, 0)
        new_state["explanation_count"][explained_name] = count + 1
        new_state["last_explained_concept"] = explained_name

        if result["queue"]:
            new_state["mode"] = "WAITING_CONTINUATION"
            new_state["queue"] = result["queue"]
            unmentioned = result.get("unmentioned_concepts", []) 
            new_state["unmentioned_concepts"] = unmentioned 
            next_q = result["queue"][0]
            if next_q in unmentioned:
                new_state["last_tutor_question_type"] = "do_you_know"
            else:
                new_state["last_tutor_question_type"] = "shall_i_explain"
        else:
            new_state["mode"] = "POST_EXPLANATION"

    # --- ìƒíƒœ 3: ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬ (IDLE ìƒíƒœ) ---
    elif current_mode == "IDLE":
        result = intelligent_tutor(
            user_input,
            new_state["explained_concepts"],
            new_state["explanation_count"]
        )
        
        # learning_pathê°€ ìˆìœ¼ë©´ new_stateì— ì €ì¥ (ì‹ ê·œ) 
        if result.get("learning_path"):
            new_state["learning_path"] = result["learning_path"]

        if result.get("error"):
            response_text = f"{result['error']}. ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆì„ê¹Œìš”?"
            new_state["mode"] = "IDLE"

        elif result.get("fallback_needed"):
            concept_name = result["concept"]
            # (ìˆ˜ì •) ì ‘ë‘ì‚¬(response_prefix)ë¡œ ë¶„ë¦¬í•˜ê³ , ìŠ¤íŠ¸ë¦¼ ë˜í•‘(wrapping) ì œê±°
            response_prefix = f"'{concept_name}'ì— ëŒ€í•´ ì œê°€ ì•„ëŠ” ì„ ì—ì„œ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”.\n\n"
            fallback_stream = generate_general_explanation(concept_name)
            
            response_stream = fallback_stream
            new_state["last_explained_concept"] = concept_name
            new_state["mode"] = "POST_EXPLANATION"

        elif result.get("needs_diagnosis"):
            response_stream = result['diagnostic_question_stream']
            
            new_state["mode"] = "WAITING_DIAGNOSTIC"
            new_state["target_concept_info"] = result["concept_info"]
            new_state["prerequisites"] = result["prerequisites"]
            new_state["queue"] = [] 
            new_state["unmentioned_concepts"] = []
            new_state["last_tutor_question_type"] = None
            new_state["primary_goal_concept"] = result["concept"]

        else: 
            response_stream = result['explanation_stream']
            
            concept_name = result["concept"]
            new_state["explained_concepts"].add(concept_name)
            count = new_state["explanation_count"].get(concept_name, 0)
            new_state["explanation_count"][concept_name] = count + 1
            new_state["last_explained_concept"] = concept_name
            new_state["mode"] = "POST_EXPLANATION"
            response_text = "\n\nğŸ’¡ ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆë‚˜ìš”?"
            
    return {"response_prefix": response_prefix, "response_stream": response_stream, "response_text": response_text, "new_state": new_state}

# 10. ë§ˆìŠ¤í„° í•¨ìˆ˜: process_turn (êµí†µ ì •ë¦¬ ë‹´ë‹¹)
def get_initial_state() -> dict:
    """Streamlit ì„¸ì…˜ ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ê¸°ë³¸ ìƒíƒœê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # (ìˆ˜ì •) JSON íŒŒì¼ì—ì„œ í”„ë¡œí•„ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    profile_data = load_profile()
    
    # (ìˆ˜ì •) ì„¸ì…˜ ìƒíƒœ ê¸°ë³¸ê°’ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    initial_state = {
        "mode": "IDLE",
        "primary_goal_concept": None,
        "target_concept_info": None,
        "prerequisites": [],
        "queue": [],
        "unmentioned_concepts": [],
        "last_tutor_question_type": None,
        "last_explained_concept": None,
        **profile_data  # (ìˆ˜ì •) ë¡œë“œëœ 'explained_concepts'ì™€ 'explanation_count'ë¥¼ ë³‘í•©
    }
    return initial_state

def process_turn(user_input: str, current_state: dict) -> dict:
    """
    ëª¨ë“  ëŒ€í™” ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ë§ˆìŠ¤í„° í•¨ìˆ˜.
    ë¼ìš°í„°ë¥¼ í˜¸ì¶œí•˜ì—¬ 'êµí†µ ì •ë¦¬' í›„ ë‹´ë‹¹ í•¸ë“¤ëŸ¬ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.
    (ìˆ˜ì •) prefix, stream, textë¥¼ ëª¨ë‘ ê²°í•©í•˜ì—¬ ìµœì¢… ìŠ¤íŠ¸ë¦¼ ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # 1) ìƒíƒœ ë³µì‚¬ ë° ê¸°ë³¸ê°’ ì„¤ì •
    response_prefix = ""
    response_stream = None
    response_text = ""
    new_state = current_state.copy() 
    
    new_state["explained_concepts"] = set(current_state.get("explained_concepts", []))
    new_state["explanation_count"] = current_state.get("explanation_count", {}).copy()
    new_state["queue"] = current_state.get("queue", []).copy()
    new_state["unmentioned_concepts"] = current_state.get("unmentioned_concepts", []).copy()
    new_state["prerequisites"] = current_state.get("prerequisites", []).copy()
    if current_state.get("target_concept_info"):
        new_state["target_concept_info"] = current_state["target_concept_info"].copy()
        
    final_stream = None
    final_text = ""
        
    try:
        log_debug(f"í˜„ì¬ ìƒíƒœ: {new_state['mode']}, í: {new_state['queue']}, ê¸°ì–µ: {new_state['explained_concepts']}")
        
        # 2) ì…ë ¥ ì²˜ë¦¬ (pending_input, í•„í„°ë§ ë“±)
        if new_state.get("pending_input"):
            user_input = new_state.pop("pending_input")
            print(f"\nğŸ“š í•™ìƒ (ë³´ë¥˜ëœ ì…ë ¥ ì²˜ë¦¬): {user_input}")
        
        if not new_state.get("pending_input") and is_system_command(user_input):
            final_text = "(ëª…ë ¹ì–´ ë˜ëŠ” ì½”ë“œ ì…ë ¥ìœ¼ë¡œ ë³´ì—¬ ë¬´ì‹œí•©ë‹ˆë‹¤. ìˆ˜í•™ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.)"
            new_state["explained_concepts"] = list(new_state["explained_concepts"])
            return {"response_text": final_text, "explanation_stream": None, "new_state": new_state} 

        if user_input.lower() in ["ì¢…ë£Œ", "exit", "quit"]:
            final_text = "ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”! ğŸ‘‹"
            new_state = get_initial_state()
            new_state["explained_concepts"] = list(new_state["explained_concepts"])
            return {"response_text": final_text, "explanation_stream": None, "new_state": new_state}
        if not user_input:
            final_text = "(ì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.)"
            new_state["explained_concepts"] = list(new_state["explained_concepts"])
            return {"response_text": final_text, "explanation_stream": None, "new_state": new_state}

        # 3) (í•µì‹¬) ë§ˆìŠ¤í„° ë¼ìš°í„° í˜¸ì¶œ
        task, topic = call_master_router(user_input, new_state) # (ìˆ˜ì •) topic ë°˜í™˜
        log_debug(f"ë§ˆìŠ¤í„° ë¼ìš°í„° ë¶„ë¥˜ ê²°ê³¼: '{task}', ì£¼ì œ: '{topic}'")

        # 4) ì‘ì—… ë¶„ë°° (ë¼ìš°íŒ…)
        if task == "greeting":
            response_stream = iter(["ì•ˆë…•í•˜ì„¸ìš”! ğŸ¤– ìˆ˜í•™ ê°œë…ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”."])
        
        elif task == "ask_problem":
            concept_for_problem = None
            if topic != "none":
                log_debug(f"ìš”ì²­ëœ ì£¼ì œ '{topic}'(ìœ¼)ë¡œ ë¬¸ì œ ìƒì„± ì‹œë„")
                concept_for_problem = topic
            else:
                last_concept = new_state.get("last_explained_concept")
                if last_concept:
                    log_debug(f"ë§ˆì§€ë§‰ í•™ìŠµ ê°œë… '{last_concept}'(ìœ¼)ë¡œ ë¬¸ì œ ìƒì„± ì‹œë„")
                    concept_for_problem = last_concept
            
            if concept_for_problem:
                # (ì‹ ê·œ) ì´ ê°œë…ì— ëŒ€í•œ ì„¤ëª…/ë¬¸ì œí’€ì´ íšŸìˆ˜ë¥¼ ê°€ì ¸ì˜´
                count = new_state.get("explanation_count", {}).get(concept_for_problem, 0)
                
                # (ìˆ˜ì •) generate_problem í˜¸ì¶œ ì‹œ count ì „ë‹¬
                problem_result = generate_problem(concept_for_problem, count)
                response_stream = problem_result["problem_stream"]
                
                if problem_result["problem_data"]:
                    # (ìˆ˜ì •) ìƒˆ ìƒíƒœì™€ ì •ë‹µ ë°ì´í„°ë¥¼ new_stateì— ì €ì¥
                    new_state["mode"] = "WAITING_PROBLEM_ANSWER" # (ì¤‘ìš”) ìƒˆ ëª¨ë“œ ì„¤ì •
                    new_state["current_problem"] = problem_result["problem_data"]
                    log_debug(f"ìƒˆ ë¬¸ì œ ìƒíƒœ ì €ì¥: {new_state['current_problem']}")
                else:
                    # ë¬¸ì œ ìƒì„± ì‹¤íŒ¨ ì‹œ
                    new_state["mode"] = "POST_EXPLANATION" 
            else:
                log_debug("ë¬¸ì œ ìƒì„± ìš”ì²­ ì‹¤íŒ¨: ì£¼ì œ ë° ë§ˆì§€ë§‰ ê°œë… ì—†ìŒ")
                response_text = "ë¨¼ì € í•™ìŠµí•  ê°œë…ì„ ì•Œë ¤ì£¼ì„¸ìš”! ì–´ë–¤ ê°œë…ì— ëŒ€í•œ ë¬¸ì œë¥¼ ë‚´ë“œë¦´ê¹Œìš”?"
                new_state["mode"] = "IDLE"

        elif task == "chitchat":
            log_debug("ì¡ë‹´ ì²˜ë¦¬ ìš”ì²­")
            response_stream = handle_chitchat(user_input)
        
        elif task == "solve_problem":
            log_debug("ë¬¸ì œ í’€ì´ ë‹µë³€ ì²˜ë¦¬ ìš”ì²­")
            
            # (ìˆ˜ì •) new_stateì—ì„œ í˜„ì¬ ë¬¸ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            problem_data = new_state.get("current_problem")
            
            if problem_data:
                # (ìˆ˜ì •) ìƒˆ í•¸ë“¤ëŸ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ í”¼ë“œë°± ìŠ¤íŠ¸ë¦¼ ìƒì„±
                response_stream = handle_solve_problem(user_input, problem_data)
                new_state["mode"] = "POST_EXPLANATION" # ì±„ì  í›„ ë‹¤ì‹œ ì¼ë°˜ ëŒ€ê¸° ëª¨ë“œ
                new_state["current_problem"] = None # (ì¤‘ìš”) í’€ì´ê°€ ëë‚¬ìœ¼ë¯€ë¡œ ë¬¸ì œ ë°ì´í„° ë¹„ìš°ê¸°
            else:
                # ë¹„ì •ìƒì ì¸ ìƒí™© (ë²„ê·¸)
                log_debug("ì˜¤ë¥˜: WAITING_PROBLEM_ANSWER ìƒíƒœì˜€ìœ¼ë‚˜ current_problem ë°ì´í„°ê°€ ì—†ìŒ")
                response_text = "ì–´ë–¤ ë¬¸ì œì— ëŒ€í•œ ë‹µì¸ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš”. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
                new_state["mode"] = "IDLE"
            
        elif task == "tutor_flow":
            log_debug("í•µì‹¬ íŠœí„° íë¦„(tutor_flow) í•¸ë“¤ëŸ¬ í˜¸ì¶œ")
            result_dict = handle_tutor_flow(user_input, new_state)
            
            response_prefix = result_dict.get("response_prefix", "")
            response_stream = result_dict.get("response_stream")
            response_text = result_dict.get("response_text", "")
            new_state = result_dict.get("new_state", new_state)

        else:
            log_debug(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¼ìš°í„° ì‘ì—…: {task}")
            response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            reset_conversation_flow(new_state)

        # 3ê°€ì§€ ìš”ì†Œ (prefix, stream, text)ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
        has_prefix = response_prefix and response_prefix.strip()
        has_stream = response_stream is not None
        has_suffix = response_text and response_text.strip()
        
        if not has_prefix and not has_stream and not has_suffix:
            log_debug("ë°˜í™˜í•  ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤. (ë¼ìš°íŒ… ì˜¤ë¥˜ ê°€ëŠ¥ì„±)")
            final_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
        elif not has_stream:
            # ìŠ¤íŠ¸ë¦¼ ì—†ì´ í…ìŠ¤íŠ¸(ì ‘ë‘ì‚¬, í›„ì†ì§ˆë¬¸)ë§Œ ìˆëŠ” ê²½ìš° (e.g. greeting)
            log_debug("ìŠ¤íŠ¸ë¦¼ ì—†ì´ prefix/suffix í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            final_text = (response_prefix if has_prefix else "") + (response_text if has_suffix else "")
            
        else:
            log_debug("ìŠ¤íŠ¸ë¦¼ê³¼ optional prefix/suffixë¥¼ ê²°í•©í•©ë‹ˆë‹¤.")
            
            def combined_stream_generator():
                # 1. Prefix (ì ‘ë‘ì‚¬)
                if has_prefix:
                    yield response_prefix
                    
                # 2. Stream (ë©”ì¸ ìŠ¤íŠ¸ë¦¼)
                stream_content = ""
                for chunk in response_stream:
                    stream_content += chunk
                    yield chunk
                
                # 3. Suffix (í›„ì† ì§ˆë¬¸)
                if has_suffix:
                    stream_content_normalized = " ".join(stream_content.split())
                    response_text_normalized = " ".join(response_text.split())
                    
                    if not stream_content_normalized.endswith(response_text_normalized):
                        log_debug("ìŠ¤íŠ¸ë¦¼ ëì— í›„ì† í…ìŠ¤íŠ¸(suffix)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                        yield response_text 
                    else:
                        log_debug("ìŠ¤íŠ¸ë¦¼ì— ì´ë¯¸ í›„ì† í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆì–´ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            final_stream = combined_stream_generator()

    # 5. ì˜ˆì™¸ ì²˜ë¦¬ (ì „ì²´ process_turn í•¨ìˆ˜ë¥¼ ê°ì‹¸ëŠ” try-except)
    except Exception as e:
        print(f"--- ğŸš¨ FATAL ERROR in process_turn ---")
        import traceback
        traceback.print_exc()
        print(f"--------------------------------------")
        final_stream = None
        final_text = f"ì£„ì†¡í•©ë‹ˆë‹¤. íŠœí„°ì™€ ëŒ€í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}. ê¸°ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
        new_state = get_initial_state() 
            
    # 6. ìµœì¢… ë°˜í™˜ (app.pyê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
        
    save_profile(new_state)
    
    if "explained_concepts" in new_state:
        new_state["explained_concepts"] = list(new_state["explained_concepts"])
             
    current_mode = new_state.get("mode")
    if current_mode not in ["WAITING_DIAGNOSTIC", "WAITING_CONTINUATION"]:
        if "target_concept_info" in new_state:
            log_debug(f"ëŒ€í™” íë¦„(mode: {current_mode})ì´ ì¢…ë£Œë˜ì–´ target_concept_infoë¥¼ ë¹„ì›ë‹ˆë‹¤.")
            new_state["target_concept_info"] = None
        
    final_state_summary = {
        k: v for k, v in new_state.items() 
        if k not in ["target_concept_info", "prerequisites"]
    }
    log_debug(f"ë°˜í™˜ ìƒíƒœ: {final_state_summary}")
        
    return {
            "explanation_stream": final_stream,
            "response_text": final_text,
            "new_state": new_state
    }